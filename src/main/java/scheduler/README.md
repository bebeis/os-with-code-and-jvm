# 10. 스케줄링: 개요
#OS/OSTEP/이론/가상화

앞선 챕터에서는 제한적 직접 실행 원리(LDE)를 통해 운영체제가 어떻게 제어권을 얻고 시분할 기법을 구현할 수 있는지를 살펴봤다. 이번에는 OS가 CPU 제어권을 얻었을 때, 어떤 프로세스를 실행시키게 할 지 결정하는 스케줄링 정책(원칙)에 대해 알아본다.

## 10.1 워크로드에 대한 가정
**워크로드**: 일련의 프로세스들이 실행하는 상황
워크로드에 관해 잘 알 수록 정책을 정교하게 결정할 수 있다. (다만, 워크로드를 정확히 예측하는 건 현실적으로 어렵다)

여기서는 비현실적인 워크로드를 가정하고, 가정을 완화시켜 나가면서 최종적으로 제대로 도악하는 스케줄링 정책을 만들게 된다.
1. 모든 작업은 같은 시간 동안 실행된다.
2. 모든 작업은 동시에 도착한다.
3. 각 작업은 시작되면 완료될 때 까지 실행된다.
4. 모든 작업은 CPU만 사용한다.(즉, 입출력을 수행하지 않는다.)
5. 각 작업의 실행 시간은 사전에 알려져 있다.

(이 다섯가지 가정은 스케줄러가 모든 것을 다 알 수 있다는 것을 의미한다.)

## 10.2 스케줄링 평가 항목
스케줄링 정책의 비교를 위해 **스케줄링 평가 항목(scheduling metric)**, 즉 평가 기준을 결정해야 한다.

**반환 시간(turnaround time)**
- 작업이 완료된 시각에서 작업이 시스템에 도착한 시간을 뺀 시간이다.
- $T_{turnaround} = T_{completion} - T_{arrival}$
    - 여기서 모든 작업은 동시에 도착하므로 $T_{arrival} = 0$
    - $T_{turnaround} = T_{completion}$
- 반환 시간은 **성능** 측면에서 평가 기준이다.

다른 평가 기준으로 **공정성(fairness)**가 있다.
- 성능과 공정성은 서로 상충되는 목표이다.
- **성능을 챙기려면** 몇몇 작업의 실행을 중지하고 **최대한 하나만 쭉 실행**해야 하는데, 결국 **공정성이 악화**된다.

## 10.3 선입선출(FIFO)
가장 기초적인 알고리즘은 `First In First Out, FIFO`이다. (== 선도착선처리, First Come First Served, FCFS)
- 단순하고 구현하기 쉽다.
- 현재 우리의 5가지 가정 하에는 매우 단순하고 구현하기 쉽다.

**ex) Task A, B, C가 거의 동시에 도착한 경우(A, B, C 순서)**
![](img/image.png)<!-- {"width":579} -->
- 세 작업의 평균 반환 시간은 (10 + 20 + 30) / 3 = 20

여기서 1번 가정(작업의 실행 시간이 모두 같지 않다고 가정)을 완화해보자. **어떤 문제가 발생**할까?

> 참고로 필자는 여기서 HTTP/1.1 -> HTTP/2에서 넘어갈 때의 프레임 인터리빙이 떠올랐다. 앞에서 큰 객체를 전송하는 바람에 뒷 객체가 오랫동한 전송되지 못하는 상황이 떠올랐고, 프레임 인터리빙 같은 방식이나, 짧은 작업을 먼저 실행하면 될 것이라 생각했다.

**예시 상황을 보자. A는 100초, B와 C는 10초 동안 실행된다고 해보자.**
![](img/image%202.png)<!-- {"width":584} -->
- 시스템의 평균 반환시간은 (100 + 110 + 120) / 3 = 110 이다.
    - 매우 길다.

이런 문제점은 **convoy effect(호위 효과)**라고 칭하며, 짧은 시간 동안 자원을 사용할 프로세스들이 자원을 오랫동안 사용하는 프로세스의 종료를 기다리는 현상을 말한다.
이런 문제를 해결하기 위해서 어떤 방법이 있을까?

## 10.4 최단 작업 우선(SJF)
**최단 작업 우선(Shortest Job First, SJF)**은 가장 짧은 실행 시간을 가진 작업을 먼저 실행시킨다.

SJF가 위에서 봤던 문제를 어느정도로 해결해 주는지 계산해보자.
![](img/image%203.png)<!-- {"width":526} -->
- 평균 반환 시간: (10 + 20 + 12) / 3 = 50
- FIFO에 비해 2배 이상 향상되었다.
  SJF는 **모든 작업이 동시에 도착한다면** **최적의** 스케줄링 알고리즘이다.

> [!NOTE] SJF의 원칙
> SJF는 평균 반환 시간이 강조되는 모든 일반적인 스케줄링 원리이다.
> 식료품 가게에서 적은 개수의 물건을 구매하는 사람이 많은 물건을 구매하는 사람 뒤에 서서 기다리는 걸 방지하기 위해, “소량 구매 계산대”를 갖추는 것과 비슷하다.

하지만 아직 우리는 4개의 가정을 완화해나가야 한다.
먼저 `가정 2`(모든 작업은 동시에 도착하는 것이 아니라 임의의 시간에 도착할 수 있다)를 완화해보자.
이러면 어떤 문제가 발생할까?

> 필자는 긴 작업 수행 중에 짧은 작업이 들어오면, 결국 짧은 작업은 나중에 수행되므로 평균 반환 시간이 길어지는 문제가 발생할 것이라고 생각했다.

**예시) A는 t=0에 도착, 100초 실행 | B와 C는 t=10에 도착, 각각 10초 실행**
![](img/image%204.png)<!-- {"width":608} -->
- B와 C가 A의 바로 뒤에 도착한다 하더라도 A가 끝날 때 까지 기다릴 수 밖에 없어, convoy 문제가 발생한다.
- 평균 반환 시간: (100 + (110 - 10) + (120 - 10)) = 310 / 3 = 103.333…

어떻게 이 문제를 해결해야 할까?

> [!NOTE] 선점형 스케줄러
> 예전의 일괄처리 시절에는 비선점(non-preemptive) 스케줄러가 개발되었다. 예전에는 각 작업이 종료될 때 까지 계속 실행했다. 모든 현대 스케줄러는 **선점**형이고, 다른 프로세스를 실행시키기 위하여 필요하면 현재 프로세스의 실행을 중단한다. 앞에서 배웠던 기법(LDE)을 통해 **문맥 교환**을 수행할 수 있게 되었기 때문이다.

## 10.5 최소 잔여시간 우선
이 문제를 해결하기 위해서는 `가정 3`(작업은 끝날 때까지 계속 실행된다)을 완화해야 한다. 타이머 인터럽트와 문맥 교환을 고려하면 B와 C가 도착했을 때 스케줄러는 다른 일을 할 수 있다.
- 작업 A를 중지하고 방금 도착한 B 또는 C를 실행하기로 결정할 수 있다. 이 경우 선점된 A는 나중에 다시 실행된다.
- 하지만 SJF는 **비선점**형 스케줄러이기 때문에 이와 같은 동작을 하지 못한다.

SJF에 선점 기능을 추가한 정책을 **최단 잔여시간 우선(Shortest Time-to-Completion First, STCF)**라고 한다.
- 언제든 새로운 작업이 시스템에 들어오면, 남아 있는 작업과 새로운 작업의 잔여 실행 시간을 계산하고 그 중 가장 작은 잔여 실행 시간을 가진 작업을 스케줄한다.

**예시**
![](img/image%205.png)<!-- {"width":565} -->
- 평균 반환 시간 : ((120-0) + (20 - 10) + (30 - 10)) / 3 = 50

새로운 가정 하에서는 STCF가 최적의 스케줄러이다.

## 10.6 새로운 평가 기준 : 응답 시간
작업의 길이를 미리 알고 있고, 작업이 오직 CPU만 사용하며, **평가 기준이 반환 시간 하나**라면 STCF는 매우 훌륭한 정책이다.
- 초기 일괄처리 컴퓨터 시스템에서는 의미있는 알고리즘이었다.

하지만 시분할 컴퓨터의 등장으로 터미널에서 작업을 진행하며 시스템에게 상호작용을 원활히 하기 위한 성능을 요구하게 되었다.
- **응답 시간(response time)**이라는 새로운 평가 기준이 생겨났다.

**응답 시간**은 작업이 도착할 때 부터 처음 스케줄 될 때까지의 시간으로 정의된다.
- $T_{response} = T_{firstrun} - T_{arrival}$
- 10.5 예시에서는
    - A: 0 - 0 = 0
    - B: 10 - 10 : 0
    - C : 20 - 10 : 10
    - 평균 3.33…

STCF를 비롯한 비슷한 류의 정책은 응답 시간이 짧다고 할 수 없다. 예를 들어 3개의 작업이 동시에 도착한 경우, 세 번째 작업은 딱 한 번 스케줄 되기 위해 먼저 실행된 두 작업이 완전히 끝날 때 까지 기다리므로 응답 시간과 상호작용 측면에서는 매우 나쁘다.

> 평균 반환 시간을 위해서는 앞에 짧은 작업들이 완전히 완료되는 것을 용인한다. 하지만 이 때문에 뒤에 있는 작업은 앞에서 스케줄되지 않는다. 응답 시간이 나빠지게 된다.

**우리는 응답 시간에 민감한 스케줄러를 만들어야 한다.**

## 10.7 라운드 로빈
응답 시간 문제를 해결하기 위해 **라운드 로빈(Round-Robin, RR)** 스케줄링 알고리즘을 도입한다.
- 작업이 끝날 때 까지 기다리지 않는다.
- 일정 시간 동안 실행한 후 실행 큐의 다음 작업으로 전환한다.
    - 이 때 작업이 실행되는 일정 시간을 **타임 슬라이스(time slice)** 또는 **스케줄링 퀀텀(scheduling quantum)**이라 부른다.
    - 타임 슬라이스의 길이는 타이머 인러텁트 주기의 배수여야 한다.

**예시) A, B, C는 동시 도착, 5초간 실행됨**
![](img/image%206.png)<!-- {"width":579} -->
- SJF 스케줄러의 평균 응답 시간: (0 + 5 + 10) / 3 = 5
- RR의 평균 응답 시간: (0 + 1 + 2) / 3 = 1

**타임 슬라이스의 길이**는 RR에서 매우 중요하다.
- **타임 슬라이스가 짧을수록** **응답 시간**을 기준으로 **RR의 성능**은 더 좋아진다.
- 그렇다고 타임 슬라이스가 너무 짧으면 **문맥 교환 비용이 커지므로 전체 성능에 큰 영향을 준다.**
- 최적의 길이를 결정해야 한다. 문맥 교환 비용을 상쇄할만큼 길어야 하지만 응답 시간이 너무 길어도 안 된다.

> [!NOTE] 비용의 상쇄(amortization)
> PS를 많이 해보신 분들은 **amortization**이라는 용어를 들어봤을 것이다.
> 한글로 상쇄라는 뜻으로, 연산에 고정 비용이 존재하는 시스템에서 흔히 사용되는 용어이다.
> 고정 비용 연산을 적게 실행하면 시스템의 비용이 감소된다.
> - 타임 슬라이스는 10ms 이고, 컨텍스트 스위칭 비용이 1ms이면 약 10%가 컨텍스트 스위칭에 사용되고 낭비된다.
> - 만약 이 비용을 상쇄하고 싶으면, 타임 슬라이스를 100ms로 늘릴 수 있다. 이러면 1% 미만의 시간이 컨텍스트 스위칭에 소모되고 타임 슬라이싱의 비용이 상쇄된다.
    >   - 다만, 이 경우 응답 시간이 늘어나겠죠?

문맥 교환(컨텍스트 스위칭) 비용에는 레지스터 저장/복원 외에도 CPU 캐시, TLB, 분기 예측 같은 하드웨어 관련 작업 정보들을 다루는 작업이 있다. 문맥 교환시 정보들이 갱신되야 하므로 매우 큰 성능 비용을 유발한다.

**적당한 길이의 응답 시간이 유일한 평가 기준**인 경우 RR은 매우 훌륭한 스케줄러이다. 하지만 **반환 시간 기준**으로는 다르다.
- A, B, C 작업이 5초간 실행되고 1초의 타임 슬라이스를 가진다고 해보자.
- A는 13, B는 14, C는 15에 종료하고 평균 14의 반환 시간을 보인다. **반환 시간은 매우 안 좋다.**
- 반환 시간이 유일한 측정 기준인 경우 RR은 최악의 정책이다. FIFO 보다 안 좋은 경우도 있다.

RR 같은 **공정**한 정책은 반환 시간과 같은 평가 기준에서 성능이 나쁘다. 불공정하게 한다면 응답 시간은 포기해야 한다.
뭔가 절충안을 찾아야 할 것 같은데, 아직 우리에겐 완화하지 않은 두 가지 가정이 남아 있다.

## 10.8 입출력 연산의 고려
`가정 4`(작업은 입출력을 하지 않는다)를 완화하자. 모든 프로그램은 입출력 작업을 수행한다.

1. 입출력 작업을 요청한 경우 스케줄러는 다음에 어떤 작업을 실행할 지 결정해야 한다.
    - 현재 실행 중인 작업은 입출력이 완료될 때 까지 CPU를 사용하지 않을 것이기 때문이다. (대기 상태가 됨)
    - 이 시간동안 실행될 다른 작업을 스케줄 해야 한다.

2. 마찬가지로 스케줄러는 입출력 완료 시에도 의사 결정을 해야 한다.
    - 인터럽트가 발생하고 운영체제가 실행되어 입출력을 요청한 프로세스를 대기 상태에서 준비 상태로 다시 이동 시키거나, 즉시 실행시키기로 결정할 수 있다.

**상황을 잘 이해하기 위해 예를 살펴보자. A, B 모두 50ms CPU 시간이 필요하지만, A는 10ms 실행 후 10ms I/O를 수행한다.**
![](img/image%207.png)<!-- {"width":526} -->
- 여기에 STCF 스케줄러(선점, 새로운 작업이 들어오면 가장 적게 남은거 실행)을 구축해보자.
- A는 5개의 10ms 작업으로 분할되고, B는 하나의 50ms CPU를 요청한다.

![](img/image%208.png)<!-- {"width":579} -->
- 일반적인 접근 방식으로, A의 각 하위 작업을 독립적인 작업으로 다룬다.
    - 시스템 시작 시 10ms 작업들과 50ms를 스케줄한다.
    - 처음에 가장 짧은 A를 선택한다. 그리고 A의 소작업이 완료되면 B만 남게되어 실행을 시작한다.
    - 이후 A의 I/O가 완료되고 A의 다음 작업이 제출되어 B를 선점하여 10ms동안 A를 실행한다.
    - 이렇게 되면, 프로세스 I/O를 기다리는 동안 CPU는 다른 프로세스에 의해 사용되어 연산의 중첩이 가능해진다.

각 CPU 버스트를 하나의 작업으로 간주함으로써, 스케줄러는 대화형 프로세스가 더 자주 유리하게 실행되는 것을 보장한다.

## 10.9 만병통치약은 없다.
마지막 가정이 하나 남아있다. 스케줄러가 각 작업의 실행 시간을 알고 있다는 가정이다. 사실 최악의 가정이다. 작업의 길이에 대해 알 수 있는 길이 없다.

다음 챕터에서는 가까운 과거를 이용해 미래를 예측하는 스케줄러(멀티 레벨 피드백 큐)를 구현한다.

> [!NOTE] 스프링 부트(내장 톰캣) + Spring MVC의 경우 사용자 요청을 어떤 스케줄링 알고리즘에 따라 처리할까?
>
> **내부적으로 사용하는 서블릿 컨테이너(Tomcat, Jetty, Undertow 등)와 OS 스케줄러**에 의존한다.
> - Tomcat은 요청을 받으면 **요청을 처리할 수 있는 스레드를 스레드 풀에서 할당**한다.
> - `org.apache.tomcat.util.threads.ThreadPoolExecutor`
> - Tomcat(또는 Jetty/Undertow)이 사용하는 스케줄링 방식은 Java ThreadPoolExecutor가 제공하는 것과 동일하다.
    >   - **작업 큐에 들어간 요청은 FIFO(선입선출) 방식**으로 처리된다.
>   - 스레드가 다 차면 큐에 쌓였다가 빈 스레드가 생기면 처리된다.
>
> **실제 실행 순서**
> - 요청이 Tomcat Acceptor 스레드에 도착
> - Acceptor → Worker 스레드 풀에 작업 제출
> - Worker 스레드 풀에서 **대기열(FIFO)** → 스레드가 요청을 하나 꺼내서 실행
> - Spring MVC 디스패처 서블릿 → 핸들러 매핑 → 컨트롤러 실행
>
> **그럼 OS 스케줄링은?**
> - Tomcat의 워커 스레드가 결국 **JVM 스레드 = OS 커널 스레드**로 매핑되므로, 실제 CPU에서 언제 실행될지는 **OS 스케줄러(CFS, MLFQ 등)**가 결정한다.
    >   - 리눅스라면 **CFS(Completely Fair Scheduler)**가 기본이다.
>   - 즉, Spring Boot → Tomcat → ThreadPoolExecutor(FIFO) → OS 스케줄러(CFS)

## 구현 코드 실행 결과
```
==================================================
Example 1: A(10), B(20), C(30), all at t=0
--------------------------------------------------
policy.name() = FIFO
=== Metrics ===
Avg turnaround = 33.333333333333336
Avg response   = 13.333333333333334

policy.name() = SJF
=== Metrics ===
Avg turnaround = 33.333333333333336
Avg response   = 13.333333333333334

policy.name() = STCF
=== Metrics ===
Avg turnaround = 33.333333333333336
Avg response   = 13.333333333333334

policy.name() = RoundRobin
=== Metrics ===
Avg turnaround = 41.666666666666664
Avg response   = 5.0

==================================================
Example 2: Convoy - A(100), B(10), C(10), all at t=0
--------------------------------------------------
policy.name() = FIFO
=== Metrics ===
Avg turnaround = 110.0
Avg response   = 70.0

policy.name() = SJF
=== Metrics ===
Avg turnaround = 50.0
Avg response   = 10.0

policy.name() = STCF
=== Metrics ===
Avg turnaround = 50.0
Avg response   = 10.0

policy.name() = RoundRobin
=== Metrics ===
Avg turnaround = 60.0
Avg response   = 5.0

==================================================
Example 3: STCF - A(100@0), B(10@10), C(10@10)
--------------------------------------------------
policy.name() = FIFO
=== Metrics ===
Avg turnaround = 103.33333333333333
Avg response   = 63.333333333333336

policy.name() = SJF
=== Metrics ===
Avg turnaround = 103.33333333333333
Avg response   = 63.333333333333336

policy.name() = STCF
=== Metrics ===
Avg turnaround = 50.0
Avg response   = 3.3333333333333335

policy.name() = RoundRobin
=== Metrics ===
Avg turnaround = 56.666666666666664
Avg response   = 1.6666666666666667
```
가정을 하나씩 풀어감에 따라, 짧은 것을 먼저 수행 하는 SJF, 그리고 STCF의 반환시간 및 RoundRobin의 응답속도가 좋다는 점을 볼 수 있다.


# 깔끔하게 요약
## 0. 평가 기준과 가정
**평가 기준**
- 반환 시간(turnaround time) - 성능
- 공정성(fairness)
    - 반환 시간과 상충된다.
- 응답 속도: 작업이 도착할 때 부터 처음 스케줄 될 때까지의 시간

1. 모든 작업은 같은 시간 동안 실행된다.
2. 모든 작업은 동시에 도착한다.
3. 각 작업은 시작되면 완료될 때 까지 실행된다.
4. 모든 작업은 CPU만 사용한다.(즉, 입출력을 수행하지 않는다.)
5. 각 작업의 실행 시간은 사전에 알려져 있다.

## 1. FIFO(비선점형)
5가지 가정하에는 공평하고 성능도 좋음.
여기서 **1번 가정을 해제하면, convoy effect 문제가 발생함**

## 2. SJF(비선점형)
convoy effect 문제를 해결하기 위해 가장 짧은 거 먼저 실행. **2~5번 가정하에** **최적임**.
여기서 **2번 가정을 해제하면(각기 다른 시간에 작업 도착)** **convoy effect 문제가 다시 발생함.**

이 문제를 해결하기 위해선 선점형 스케줄러가 필요.

## 3. STCF(Shortest Time-to-Completion First, 선점형)
**SJF에 선점 기능을 추가한 버전**. 새로운 작업이 시스템에 들어오면, 남아 있는 작업과 새로운 작업의 잔여 실행 시간을 계산하고 그 중 가장 작은 잔여 실행 시간을 가진 작업을 스케줄한다.

**3~5번 가정하에 최적이다.**

시분할 컴퓨터의 등장으로, 터미널 상호작용이 생겨나며 **응답 시간**이라는 평가 기준이 필요해짐.
**FIFO, SJF, STCF 모두 응답시간은 짧지 않다.** 예를 들어 3개의 작업이 동시에 도착한 경우, 세 번째 작업은 딱 한 번 스케줄 되기 위해 먼저 실행된 두 작업이 완전히 끝날 때 까지 기다리므로 응답 시간과 상호작용 측면에서는 매우 나쁘다.

정리하면
- **응답 시간은 매우 안좋음**
- **반환 시간은 좋음**

## 4. 라운드 로빈
**응답 시간 문제를 해결**하기 위해 등장. 작업이 끝날 때 까지 기다리지 않고, 일정 시간 동안 실행한 후 실행 큐의 다음 작업으로 offer된다. (3번 가정 이제 고려하지 않아도 됨)
- 이 때 작업이 실행되는 일정 시간을 **타임 슬라이스(time slice)** 또는 **스케줄링 퀀텀(scheduling quantum)**이라 부른다. (타이머 인터럽트의 배수)
- **타임 슬라이스가 짧을수록 응답 시간은 짧아진다**
- 그렇다고 타임 슬라이스가 너무 짧으면 **문맥 교환 비용이 커지므로 전체 성능에 큰 영향을 준다.**
- 최적의 길이를 결정해야 한다.

**적당한 길이의 응답 시간이 유일한 평가 기준**인 경우 RR은 매우 훌륭한 스케줄러이다. 하지만 **반환 시간 기준**으로는 다르다.
- A, B, C 작업이 5초간 실행되고 1초의 타임 슬라이스를 가진다고 해보자.
- A는 13, B는 14, C는 15에 종료하고 평균 14의 반환 시간을 보인다. **반환 시간은 매우 안 좋다.**

정리하면
- **응답 시간, 공정성은 좋음**
- **반환 시간은 매우 안좋음**

이를 어떻게 절충해야 할까? -> 사실 대부분 프로그램은 입출력 작업을 수행한다. 이를 고려한다면?

## 5. 입출력 작업
`가정 4`(작업은 입출력을 하지 않는다)를 완화하자. 모든 프로그램은 입출력 작업을 수행한다.
스케줄러는 다음과 같은 일을 수행해야 한다.
1. 입출력 작업을 요청한 경우 스케줄러는 다음에 어떤 작업을 실행할 지 결정해야 한다.
2. 입출력 완료 시에도 스케줄러는 의사 결정을 해야 한다. (READY? RUNNING?)

각 CPU 버스트를 하나의 작업으로 간주한다. 예를 들어 CPU 작업이 50s 이지만 중간에 IO 작업이 있다면 IO 작업 사이에 있는 CPU 버스트를 하나의 작업으로 간주해버린다.
이렇게 하면 IO 작업 때 다른 프로세스를 실행시킬 수 있고 스케줄러는 대화형 프로세스가 더 자주 유리하게 실행되는 것을 보장한다.

## 6. 남은 가정
5번 가정이 남았다. “각 작업의 실행 시간은 사전에 알려져 있다.”
우리는 미래를 알 수 없다. 하지만 과거의 패턴을 통해 미래를 예측하는 것은 가능하다. 다음 챕터에서 이어서…
